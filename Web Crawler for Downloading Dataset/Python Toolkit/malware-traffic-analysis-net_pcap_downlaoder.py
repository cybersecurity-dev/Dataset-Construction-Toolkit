import os
import sys
import requests

from zipfile import ZipFile
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse

def unzip_all_password_protected(directory, extract_dir, password):
  for filename in os.listdir(directory):
    # Split the filename at hyphens
    parts = filename.split("-")
    # Extract the date part (first 3 elements)
    date_str = "-".join(parts[:3])
    pwd_left_appendix = date_str.replace("-", "")
    password_new = password + pwd_left_appendix
    if filename.endswith(".zip"):
      filepath = os.path.join(directory, filename)
      try:
        with ZipFile(filepath, 'r') as zip_ref:
          zip_ref.extractall(extract_dir, pwd=password_new.encode())
          print(f"Successfully unzipped: {filename}")
      except Exception as e:
        print(f"Error unzipping {filename}: {e}")

def download_zip(url, session, download_folder):
    response = session.get(url)
    filename = os.path.join(download_folder, url.split('/')[-1])
    
    with open(filename, 'wb') as file:
        file.write(response.content)
    print(f"Downloaded: {filename}")

def get_all_links(url, session):
    response = session.get(url)
    soup = BeautifulSoup(response.content, 'html.parser')
    links = []
    
    for link in soup.find_all('a', href=True):
        full_url = urljoin(url, link['href'])
        links.append(full_url)
        
    return links

def download_zip_files_from_website(start_url, download_folder):
    if not os.path.exists(download_folder):
        os.makedirs(download_folder)
    
    visited_urls = set()
    urls_to_visit = [start_url]
    
    session = requests.Session()
    
    while urls_to_visit:
        current_url = urls_to_visit.pop(0)
        
        if current_url in visited_urls:
            continue
        visited_urls.add(current_url)
        
        try:
            links = get_all_links(current_url, session)
            for link in links:
                if link.endswith('.zip'):
                    download_zip(link, session, download_folder)
                elif urlparse(start_url).netloc == urlparse(link).netloc:
                    urls_to_visit.append(link)
        except Exception as e:
            print(f"Failed to process {current_url}: {e}")

def main(start_url, download_dir, extract_dir, password_prefix):    
    download_zip_files_from_website(start_url, download_dir)
    unzip_all_password_protected(download_dir, extract_dir, password_prefix)

if __name__ == "__main__":
  start_url = "https://www.malware-traffic-analysis.net/"  # Replace with the target website URL
  password_prefix = 'infected_'

  # Check if a parameter is provided
  if len(sys.argv) == 3 :
    download_dir = sys.argv[1]
    print(f"Download Directory:\t{download_dir}")
    
    extract_dir = sys.argv[2]
    print(f"Extract Directory:\t{extract_dir}")
    main(start_url, download_dir, extract_dir, password_prefix)
  else:
    print("No download directory and extract directory provided.")